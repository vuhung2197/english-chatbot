import pool from '../db.js';
import { getEmbedding } from '../services/embeddingVector.js';
import { StatusCodes } from 'http-status-codes';
import '../bootstrap/env.js';
import OpenAI from 'openai';
import {
  multiStageRetrieval,
  semanticClustering,
  multiHopReasoning,
  fuseContext,
  adaptiveRetrieval,
  rerankContext
} from '../services/advancedRAGFixed.js';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

/**
 * Advanced Chat Controller v·ªõi Multi-Chunk Reasoning
 * Gi·∫£i quy·∫øt c√¢u h·ªèi ph·ª©c t·∫°p c·∫ßn k·∫øt h·ª£p nhi·ªÅu ngu·ªìn th√¥ng tin
 */

/**
 * Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n AI tr·∫£ l·ªùi th√†nh Markdown v·ªõi c·∫•u tr√∫c t·ªët h∆°n
 */
function toAdvancedMarkdown(text) {
  if (!text) return '';

  const paragraphs = text.split(/\n{2,}/);
  let markdown = '';

  for (const para of paragraphs) {
    const trimmed = para.trim();
    if (!trimmed) continue;

    // Detect headers
    if (trimmed.match(/^#{1,6}\s/)) {
      markdown += `${trimmed}\n\n`;
      continue;
    }

    // Detect lists
    if (trimmed.startsWith('- ') || trimmed.startsWith('* ') || /^[‚Ä¢\-+]\s/.test(trimmed)) {
      const points = trimmed
        .split(/(?:^|\n)[‚Ä¢\-+*]?\s*/)
        .map(p => p.trim())
        .filter(p => p.length > 0);
      
      points.forEach(point => {
        markdown += `- ${point}\n`;
      });
      markdown += '\n';
      continue;
    }

    // Detect code blocks
    if (trimmed.startsWith('```')) {
      markdown += `${trimmed}\n\n`;
      continue;
    }

    // Regular paragraph
    markdown += `${trimmed}\n\n`;
  }

  return markdown.trim();
}

/**
 * Advanced Chat API v·ªõi Multi-Chunk Reasoning
 */
export async function advancedChat(req, res) {
  const { message, model } = req.body;
  const userId = req.user?.id;

  if (!message) {
    return res.status(StatusCodes.BAD_REQUEST).json({ 
      reply: 'No message!',
      reasoning_steps: [],
      chunks_used: []
    });
  }

  try {
    console.log('üß† Advanced RAG processing:', message);
    
    // 1. T·∫°o embedding cho c√¢u h·ªèi
    let questionEmbedding;
    try {
      questionEmbedding = await getEmbedding(message);
    } catch (error) {
      console.error('‚ùå L·ªói t·∫°o embedding:', error);
      return res.json({ 
        reply: 'Kh√¥ng th·ªÉ x·ª≠ l√Ω c√¢u h·ªèi n√†y!',
        reasoning_steps: [],
        chunks_used: []
      });
    }

    // 2. Adaptive Retrieval - ƒêi·ªÅu ch·ªânh retrieval d·ª±a tr√™n ƒë·ªô ph·ª©c t·∫°p
    const retrievalParams = await adaptiveRetrieval(message, questionEmbedding);
    console.log('üìä Retrieval params:', retrievalParams);

    // 3. Multi-Stage Retrieval - L·∫•y chunks theo nhi·ªÅu giai ƒëo·∫°n
    const allChunks = await multiStageRetrieval(
      questionEmbedding, 
      message, 
      retrievalParams.maxChunks
    );
    
    if (allChunks.length === 0) {
      await logUnanswered(message);
      return res.json({
        reply: 'T√¥i ch∆∞a c√≥ ki·∫øn th·ª©c ph√π h·ª£p ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.',
        reasoning_steps: ['Kh√¥ng t√¨m th·∫•y chunks ph√π h·ª£p'],
        chunks_used: []
      });
    }

    console.log(`üìö Retrieved ${allChunks.length} chunks`);

    // 4. Semantic Clustering - Nh√≥m chunks theo ch·ªß ƒë·ªÅ
    let clusters = [];
    try {
      clusters = await semanticClustering(allChunks, questionEmbedding);
      console.log(`üîó Created ${clusters.length} semantic clusters`);
    } catch (error) {
      console.error('‚ùå Error in semantic clustering:', error);
      clusters = [allChunks]; // Fallback to single cluster
    }

    // 5. Multi-Hop Reasoning - T√¨m m·ªëi li√™n k·∫øt gi·ªØa chunks
    let reasoningChains = [];
    if (retrievalParams.useMultiHop) {
      try {
        reasoningChains = await multiHopReasoning(
          allChunks.slice(0, 5), 
          questionEmbedding, 
          message
        );
        console.log(`üîó Created ${reasoningChains.length} reasoning chains`);
      } catch (error) {
        console.error('‚ùå Error in multi-hop reasoning:', error);
        reasoningChains = []; // Continue without reasoning
      }
    }

    // 6. Context Re-ranking - S·∫Øp x·∫øp l·∫°i context
    let rerankedChunks = allChunks;
    try {
      rerankedChunks = rerankContext(allChunks, questionEmbedding, message);
      console.log('üìà Reranked chunks by relevance and coherence');
    } catch (error) {
      console.error('‚ùå Error in context re-ranking:', error);
      // Use original chunks
    }

    // 7. Context Fusion - K·∫øt h·ª£p th√¥ng minh
    let fusedContext = '';
    try {
      fusedContext = fuseContext(rerankedChunks, reasoningChains, message);
      console.log('üîó Fused context length:', fusedContext.length);
      
      // Debug: Log context preview ƒë·ªÉ ki·ªÉm tra
      console.log('üìÑ Context preview:', fusedContext.substring(0, 200) + '...');
    } catch (error) {
      console.error('‚ùå Error in context fusion:', error);
      // Fallback to simple context
      fusedContext = rerankedChunks.map(c => `**${c.title}**: ${c.content}`).join('\n\n');
    }

    // 8. Advanced System Prompt
    const systemPrompt = `B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp v·ªõi kh·∫£ nƒÉng ph√¢n t√≠ch v√† k·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn.

H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
1. Ph√¢n t√≠ch c√¢u h·ªèi ƒë·ªÉ x√°c ƒë·ªãnh c√°c kh√≠a c·∫°nh c·∫ßn tr·∫£ l·ªùi
2. K·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn m·ªôt c√°ch logic
3. T·∫°o c√¢u tr·∫£ l·ªùi c√≥ c·∫•u tr√∫c r√µ r√†ng v·ªõi c√°c ph·∫ßn:
   - T√≥m t·∫Øt ch√≠nh
   - Chi ti·∫øt t·ª´ng kh√≠a c·∫°nh
   - K·∫øt lu·∫≠n v√† li√™n k·∫øt
4. S·ª≠ d·ª•ng markdown ƒë·ªÉ format c√¢u tr·∫£ l·ªùi
5. N·∫øu th√¥ng tin kh√¥ng ƒë·ªß, h√£y n√≥i r√µ v√† ƒë·ªÅ xu·∫•t h∆∞·ªõng t√¨m hi·ªÉu th√™m`;

    // 9. G·ªçi LLM v·ªõi context n√¢ng cao - v·ªõi timeout protection
    const t0 = Date.now();
    let reply = '';
    try {
      // Set timeout for LLM call
      const timeoutPromise = new Promise((_, reject) => 
        setTimeout(() => reject(new Error('LLM call timeout')), 30000)
      );
      
      const llmPromise = askAdvancedChatGPT(message, fusedContext, systemPrompt, model);
      reply = await Promise.race([llmPromise, timeoutPromise]);
    } catch (error) {
      console.error('‚ùå Error in LLM call:', error);
      reply = 'Xin l·ªói, t√¥i g·∫∑p s·ª± c·ªë khi x·ª≠ l√Ω c√¢u h·ªèi ph·ª©c t·∫°p n√†y. Vui l√≤ng th·ª≠ l·∫°i v·ªõi c√¢u h·ªèi ƒë∆°n gi·∫£n h∆°n.';
    }
    
    const t1 = Date.now();
    console.log('‚è±Ô∏è Advanced RAG processing time:', t1 - t0, 'ms');

    // 10. T·∫°o reasoning steps ƒë·ªÉ debug
    const reasoningSteps = [
      `Retrieved ${allChunks.length} chunks using multi-stage retrieval`,
      `Created ${clusters.length} semantic clusters`,
      `Generated ${reasoningChains.length} reasoning chains`,
      `Fused context with ${fusedContext.length} characters`,
      `Generated response using advanced RAG`
    ];

    // 11. Ghi l·ªãch s·ª≠ (kh√¥ng c√≥ metadata column)
    if (userId) {
      await pool.execute(
        'INSERT INTO user_questions (user_id, question, bot_reply, is_answered) VALUES (?, ?, ?, ?)',
        [userId, message, reply, true]
      );
    }

    res.json({ 
      reply: toAdvancedMarkdown(reply),
      reasoning_steps: reasoningSteps,
      chunks_used: rerankedChunks.map(c => ({
        id: c.id,
        title: c.title,
        score: c.final_score || c.score,
        stage: c.retrieval_stage
      })),
      metadata: {
        total_chunks: allChunks.length,
        clusters: clusters.length,
        reasoning_chains: reasoningChains.length,
        processing_time: t1 - t0
      }
    });

  } catch (err) {
    console.error('‚ùå Advanced RAG error:', err);
    res.json({ 
      reply: 'Bot ƒëang g·∫∑p s·ª± c·ªë v·ªõi c√¢u h·ªèi ph·ª©c t·∫°p n√†y. Vui l√≤ng th·ª≠ l·∫°i!',
      reasoning_steps: ['Error in advanced processing'],
      chunks_used: []
    });
  }
}

/**
 * G·ªçi OpenAI v·ªõi context n√¢ng cao
 */
async function askAdvancedChatGPT(question, context, systemPrompt, model) {
  // Gi·ªõi h·∫°n ƒë·ªô d√†i context ƒë·ªÉ tr√°nh l·ªói JSON parsing
  const maxContextLength = 8000;
  const truncatedContext = context.length > maxContextLength 
    ? context.substring(0, maxContextLength) + '...' 
    : context;

  const prompt = `# C√¢u h·ªèi: ${question}

# Th√¥ng tin tham kh·∫£o:
${truncatedContext}

# H∆∞·ªõng d·∫´n:
H√£y ph√¢n t√≠ch c√¢u h·ªèi v√† s·ª≠ d·ª•ng th√¥ng tin tham kh·∫£o ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi to√†n di·ªán. 
K·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn m·ªôt c√°ch logic v√† c√≥ c·∫•u tr√∫c.`;

  // Validate v√† clean messages
  const messages = [
    { 
      role: 'system', 
      content: (systemPrompt || '').substring(0, 4000) // Gi·ªõi h·∫°n system prompt
    },
    { 
      role: 'user', 
      content: prompt.substring(0, 12000) // Gi·ªõi h·∫°n user prompt
    }
  ];

  // Validate model name
  const validModel = model && typeof model === 'string' ? model : 'gpt-4o';

  const response = await openai.chat.completions.create({
    model: validModel,
    messages,
    temperature: 0.3,
    max_tokens: 1000
  });

  return response.choices[0].message.content.trim();
}

/**
 * Log unanswered questions
 */
async function logUnanswered(question) {
  try {
    await pool.execute(
      'INSERT INTO unanswered_questions (question, created_at) VALUES (?, NOW())',
      [question]
    );
  } catch (err) {
    console.error('‚ùå L·ªói log unanswered:', err);
  }
}

/**
 * Get advanced RAG statistics
 */
export async function getAdvancedRAGStats(req, res) {
  try {
    const [stats] = await pool.execute(`
      SELECT 
        COUNT(*) as total_questions,
        AVG(JSON_EXTRACT(metadata, '$.chunks_used')) as avg_chunks,
        AVG(JSON_EXTRACT(metadata, '$.processing_time')) as avg_processing_time,
        COUNT(CASE WHEN JSON_EXTRACT(metadata, '$.reasoning_chains') > 0 THEN 1 END) as complex_questions
      FROM user_questions 
      WHERE metadata IS NOT NULL
    `);

    res.json({
      success: true,
      stats: stats[0]
    });
  } catch (err) {
    console.error('‚ùå L·ªói get stats:', err);
    res.status(500).json({ success: false, error: err.message });
  }
}

export default {
  advancedChat,
  getAdvancedRAGStats
};
