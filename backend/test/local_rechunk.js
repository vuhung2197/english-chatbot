import mysql from 'mysql2/promise';
import { academicChunking, caseStudyChunking } from '../utils/advancedChunking.js';
import { createHash } from '../utils/hash.js';
import { getEmbedding } from '../services/embeddingVector.js';

// Táº¡o connection trá»±c tiáº¿p vá»›i localhost (Docker MySQL)
const pool = mysql.createPool({
  host: 'localhost',
  user: 'root',
  password: '123456',
  database: 'chatbot',
  port: 3307, // Docker MySQL port
  charset: 'utf8mb4',
});

/**
 * Re-chunk toÃ n bá»™ knowledge base vá»›i thuáº­t toÃ¡n má»›i
 */
async function reChunkAllKnowledgeLocal() {
  console.log('ðŸš€ Starting re-chunking of all knowledge with advanced algorithm...');
  
  try {
    // Test connection
    const [testResult] = await pool.execute('SELECT 1 as test');
    console.log('âœ… Database connected:', testResult[0].test);

    // Láº¥y táº¥t cáº£ knowledge base
    const [rows] = await pool.execute(
      'SELECT id, title, content FROM knowledge_base WHERE content IS NOT NULL AND content != ""'
    );

    console.log(`ðŸ“š Found ${rows.length} knowledge records`);

    if (rows.length === 0) {
      console.log('âš ï¸  No records found to process');
      return;
    }

    let totalProcessed = 0;
    let totalSkipped = 0;
    let totalErrors = 0;

    for (const row of rows) {
      try {
        console.log(`\nðŸ“ Processing: ${row.title}`);
        
        // XÃ³a chunks cÅ©
        await pool.execute('DELETE FROM knowledge_chunks WHERE parent_id = ?', [row.id]);
        console.log(`ðŸ—‘ï¸  Deleted old chunks for knowledge ${row.id}`);

        // Chá»n thuáº­t toÃ¡n chunking
        let chunks;
        if (row.content.toLowerCase().includes('case study') || 
            row.content.toLowerCase().includes('vÃ­ dá»¥') ||
            row.content.toLowerCase().includes('á»©ng dá»¥ng')) {
          chunks = caseStudyChunking(row.content);
          console.log('ðŸ“š Using case study chunking');
        } else {
          chunks = academicChunking(row.content);
          console.log('ðŸ“– Using academic chunking');
        }

        console.log(`ðŸ“Š Generated ${chunks.length} semantic chunks`);

        let processedChunks = 0;
        let skippedChunks = 0;
        let errorChunks = 0;

        for (const chunk of chunks) {
          try {
            const hash = createHash(chunk.content);
            
            // Kiá»ƒm tra náº¿u Ä‘Ã£ tá»“n táº¡i chunk nÃ y
            const [exists] = await pool.execute(
              'SELECT id FROM knowledge_chunks WHERE hash = ? LIMIT 1',
              [hash]
            );
            
            if (exists.length > 0) {
              skippedChunks++;
              console.log(`â­ï¸  Skipped existing chunk (${chunk.metadata.wordCount} words)`);
              continue;
            }

            // Táº¡o embedding
            const embedding = await getEmbedding(chunk.content);
            
            // LÆ°u chunk vá»›i metadata
            await pool.execute(
              `INSERT INTO knowledge_chunks 
                (parent_id, title, content, embedding, token_count, hash) 
               VALUES (?, ?, ?, ?, ?, ?)`,
              [row.id, row.title, chunk.content, JSON.stringify(embedding), chunk.metadata.wordCount, hash]
            );

            processedChunks++;
            console.log(`âœ… Processed chunk ${processedChunks}/${chunks.length} (${chunk.metadata.wordCount} words, ${chunk.metadata.boundary} boundary)`);
            
          } catch (error) {
            errorChunks++;
            console.error(`âŒ Error processing chunk:`, error.message);
          }
        }

        totalProcessed += processedChunks;
        totalSkipped += skippedChunks;
        totalErrors += errorChunks;

        console.log(`ðŸ“ˆ Summary for ${row.title}:`);
        console.log(`   - Processed: ${processedChunks} chunks`);
        console.log(`   - Skipped: ${skippedChunks} chunks`);
        console.log(`   - Errors: ${errorChunks} chunks`);

      } catch (error) {
        console.error(`âŒ Error processing knowledge ${row.id}:`, error.message);
        totalErrors++;
      }
    }

    console.log(`\nðŸŽ‰ Re-chunking completed!`);
    console.log(`   - Total processed: ${totalProcessed} chunks`);
    console.log(`   - Total skipped: ${totalSkipped} chunks`);
    console.log(`   - Total errors: ${totalErrors} chunks`);

    return {
      total: totalProcessed + totalSkipped + totalErrors,
      processed: totalProcessed,
      skipped: totalSkipped,
      errors: totalErrors
    };

  } catch (error) {
    console.error('âŒ Fatal error in re-chunking:', error);
    throw error;
  } finally {
    await pool.end();
    console.log('ðŸ”Œ Database connection closed');
  }
}

// Cháº¡y re-chunking
reChunkAllKnowledgeLocal()
  .then((result) => {
    console.log('\nðŸŽ‰ Re-chunking completed successfully!');
    console.log(`ðŸ“Š Final Results:`);
    console.log(`   - Total chunks created: ${result.total}`);
    console.log(`   - Successfully processed: ${result.processed}`);
    console.log(`   - Skipped (already existed): ${result.skipped}`);
    console.log(`   - Errors: ${result.errors}`);
    
    if (result.errors > 0) {
      console.log('\nâš ï¸  Some chunks had errors. Check the logs above for details.');
    } else {
      console.log('\nâœ… All chunks processed successfully!');
    }
    
    process.exit(0);
  })
  .catch((error) => {
    console.error('\nðŸ’¥ Fatal error:', error);
    process.exit(1);
  });
