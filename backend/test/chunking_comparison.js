import { splitIntoSemanticChunks } from '../utils/chunking.js';
import { advancedSemanticChunking, academicChunking, caseStudyChunking } from '../utils/advancedChunking.js';

// N·ªôi dung m·∫´u t·ª´ NLP document
const sampleContent = `
üìñ T√†i li·ªáu nghi√™n c·ª©u: ·ª®ng d·ª•ng th·ª±c t·∫ø c·ªßa NLP (Natural Language Processing)

1. Gi·ªõi thi·ªáu v·ªÅ NLP v√† t·∫ßm quan tr·ªçng
X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (Natural Language Processing ‚Äì NLP) l√† m·ªôt nh√°nh quan tr·ªçng c·ªßa tr√≠ tu·ªá nh√¢n t·∫°o (AI), t·∫≠p trung v√†o vi·ªác cho ph√©p m√°y t√≠nh c√≥ kh·∫£ nƒÉng hi·ªÉu, ph√¢n t√≠ch, sinh v√† t∆∞∆°ng t√°c v·ªõi ng√¥n ng·ªØ c·ªßa con ng∆∞·ªùi. Ng√¥n ng·ªØ l√† m·ªôt trong nh·ªØng h√¨nh th·ª©c giao ti·∫øp ph·ª©c t·∫°p nh·∫•t, mang theo nhi·ªÅu t·∫ßng √Ω nghƒ©a, s·∫Øc th√°i v√† ng·ªØ c·∫£nh.

Vi·ªác d·∫°y m√°y t√≠nh hi·ªÉu ng√¥n ng·ªØ gi·ªëng con ng∆∞·ªùi l√† m·ªôt th√°ch th·ª©c l·ªõn, nh∆∞ng c≈©ng l√† m·ªôt trong nh·ªØng h∆∞·ªõng ph√°t tri·ªÉn mang l·∫°i gi√° tr·ªã th·ª±c ti·ªÖn cao nh·∫•t cho AI.

T·ª´ nh·ªØng ng√†y ƒë·∫ßu ch·ªâ l√† c√°c h·ªá th·ªëng ph√¢n t√≠ch t·ª´ kh√≥a ƒë∆°n gi·∫£n, NLP ng√†y nay ƒë√£ ti·∫øn t·ªõi c√°c m√¥ h√¨nh ng√¥n ng·ªØ quy m√¥ l·ªõn (Large Language Models ‚Äì LLMs) nh∆∞ GPT, PaLM, LLaMA, Claude‚Ä¶ Nh·ªØng h·ªá th·ªëng n√†y c√≥ th·ªÉ hi·ªÉu ng√¥n ng·ªØ ·ªü m·ª©c ƒë·ªô ng·ªØ c·∫£nh cao, t·∫°o ra vƒÉn b·∫£n gi·ªëng con ng∆∞·ªùi v√† h·ªó tr·ª£ h√†ng lo·∫°t ·ª©ng d·ª•ng th·ª±c t·∫ø trong kinh doanh, y t·∫ø, gi√°o d·ª•c, t√†i ch√≠nh v√† ƒë·ªùi s·ªëng h·∫±ng ng√†y.

Theo th·ªëng k√™ c·ªßa MarketsandMarkets, th·ªã tr∆∞·ªùng NLP to√†n c·∫ßu d·ª± ki·∫øn ƒë·∫°t h∆°n 40 t·ª∑ USD v√†o nƒÉm 2025, v·ªõi t·ªëc ƒë·ªô tƒÉng tr∆∞·ªüng k√©p h√†ng nƒÉm (CAGR) kho·∫£ng 20%. ƒêi·ªÅu n√†y cho th·∫•y t·∫ßm quan tr·ªçng ng√†y c√†ng tƒÉng c·ªßa NLP trong m·ªçi lƒ©nh v·ª±c.

2. Chatbot v√† Tr·ª£ l√Ω ·∫£o
B·ªëi c·∫£nh
Tr∆∞·ªõc ƒë√¢y, d·ªãch v·ª• chƒÉm s√≥c kh√°ch h√†ng ch·ªß y·∫øu d·ª±a v√†o t·ªïng ƒë√†i vi√™n ho·∫∑c nh√¢n vi√™n tr·ª±c tuy·∫øn. ƒêi·ªÅu n√†y v·ª´a t·ªën k√©m chi ph√≠ nh√¢n s·ª± v·ª´a kh√≥ m·ªü r·ªông quy m√¥ 24/7. NLP ƒë√£ m·ªü ra k·ª∑ nguy√™n m·ªõi v·ªõi chatbot v√† tr·ª£ l√Ω ·∫£o c√≥ kh·∫£ nƒÉng giao ti·∫øp t·ª± nhi√™n v·ªõi con ng∆∞·ªùi.

·ª®ng d·ª•ng th·ª±c t·∫ø
Tr·ª£ l√Ω ·∫£o c√° nh√¢n: Siri (Apple), Alexa (Amazon), Google Assistant c√≥ kh·∫£ nƒÉng nh·∫≠n l·ªánh b·∫±ng gi·ªçng n√≥i ƒë·ªÉ ƒë·∫∑t l·ªãch, ph√°t nh·∫°c, ƒëi·ªÅu khi·ªÉn thi·∫øt b·ªã IoT.

Chatbot doanh nghi·ªáp: H·ªá th·ªëng h·ªó tr·ª£ kh√°ch h√†ng c·ªßa ng√¢n h√†ng, h√£ng h√†ng kh√¥ng, th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠. V√≠ d·ª•: Viettel, VNPT, Shopee s·ª≠ d·ª•ng chatbot ƒë·ªÉ tr·∫£ l·ªùi t·ª± ƒë·ªông c√°c c√¢u h·ªèi c∆° b·∫£n.

D·ªãch v·ª• y t·∫ø t·ª´ xa: Chatbot h·ªó tr·ª£ ch·∫©n ƒëo√°n ban ƒë·∫ßu, h∆∞·ªõng d·∫´n b·ªánh nh√¢n tr∆∞·ªõc khi g·∫∑p b√°c sƒ©.

Case study
Bank of America tri·ªÉn khai chatbot Erica, h·ªó tr·ª£ h∆°n 50 tri·ªáu kh√°ch h√†ng ki·ªÉm tra s·ªë d∆∞, thanh to√°n h√≥a ƒë∆°n, t∆∞ v·∫•n t√†i ch√≠nh.

Duolingo d√πng NLP ƒë·ªÉ x√¢y d·ª±ng chatbot h·ªôi tho·∫°i gi√∫p ng∆∞·ªùi h·ªçc ngo·∫°i ng·ªØ luy·ªán t·∫≠p v·ªõi ng·ªØ c·∫£nh g·∫ßn nh∆∞ th·∫≠t.

L·ª£i √≠ch v√† th√°ch th·ª©c
L·ª£i √≠ch: Ti·∫øt ki·ªám chi ph√≠, tƒÉng t·ªëc ƒë·ªô ph·∫£n h·ªìi, h·ªó tr·ª£ quy m√¥ l·ªõn.
Th√°ch th·ª©c: C·∫ßn hi·ªÉu ng·ªØ c·∫£nh ph·ª©c t·∫°p, x·ª≠ l√Ω m·ªâa mai, ·∫©n d·ª• v√† c·∫£m x√∫c con ng∆∞·ªùi.
`;

console.log('üîç So s√°nh thu·∫≠t to√°n chia chunk\n');

// Test thu·∫≠t to√°n c≈©
console.log('üìä THU·∫¨T TO√ÅN C≈® (splitIntoSemanticChunks):');
const oldChunks = splitIntoSemanticChunks(sampleContent, 100);
console.log(`S·ªë chunks: ${oldChunks.length}`);
oldChunks.forEach((chunk, index) => {
  const wordCount = chunk.split(/\s+/).length;
  const isComplete = chunk.endsWith('.') || chunk.endsWith('!') || chunk.endsWith('?');
  console.log(`\nChunk ${index + 1} (${wordCount} t·ª´, ho√†n ch·ªânh: ${isComplete}):`);
  console.log(`"${chunk.substring(0, 150)}${chunk.length > 150 ? '...' : ''}"`);
});

console.log('\n' + '='.repeat(80) + '\n');

// Test thu·∫≠t to√°n m·ªõi - Academic
console.log('üìä THU·∫¨T TO√ÅN M·ªöI (Academic Chunking):');
const newChunks = academicChunking(sampleContent);
console.log(`S·ªë chunks: ${newChunks.length}`);
newChunks.forEach((chunk, index) => {
  console.log(`\nChunk ${index + 1} (${chunk.metadata.wordCount} t·ª´, ${chunk.metadata.sentenceCount} c√¢u):`);
  console.log(`Boundary: ${chunk.metadata.boundary}, Complete: ${chunk.metadata.isComplete}`);
  console.log(`"${chunk.content.substring(0, 150)}${chunk.content.length > 150 ? '...' : ''}"`);
});

console.log('\n' + '='.repeat(80) + '\n');

// Test thu·∫≠t to√°n m·ªõi - Case Study
console.log('üìä THU·∫¨T TO√ÅN M·ªöI (Case Study Chunking):');
const caseStudyChunks = caseStudyChunking(sampleContent);
console.log(`S·ªë chunks: ${caseStudyChunks.length}`);
caseStudyChunks.forEach((chunk, index) => {
  console.log(`\nChunk ${index + 1} (${chunk.metadata.wordCount} t·ª´, ${chunk.metadata.sentenceCount} c√¢u):`);
  console.log(`Boundary: ${chunk.metadata.boundary}, Complete: ${chunk.metadata.isComplete}`);
  console.log(`"${chunk.content.substring(0, 150)}${chunk.content.length > 150 ? '...' : ''}"`);
});

console.log('\n' + '='.repeat(80) + '\n');

// Th·ªëng k√™ so s√°nh
console.log('üìà TH·ªêNG K√ä SO S√ÅNH:');
console.log(`Thu·∫≠t to√°n c≈©: ${oldChunks.length} chunks`);
console.log(`Academic chunking: ${newChunks.length} chunks`);
console.log(`Case study chunking: ${caseStudyChunks.length} chunks`);

const oldAvgWords = oldChunks.reduce((sum, chunk) => sum + chunk.split(/\s+/).length, 0) / oldChunks.length;
const newAvgWords = newChunks.reduce((sum, chunk) => sum + chunk.metadata.wordCount, 0) / newChunks.length;
const caseAvgWords = caseStudyChunks.reduce((sum, chunk) => sum + chunk.metadata.wordCount, 0) / caseStudyChunks.length;

console.log(`\nTrung b√¨nh t·ª´/chunk:`);
console.log(`- Thu·∫≠t to√°n c≈©: ${oldAvgWords.toFixed(1)} t·ª´`);
console.log(`- Academic: ${newAvgWords.toFixed(1)} t·ª´`);
console.log(`- Case study: ${caseAvgWords.toFixed(1)} t·ª´`);

const oldComplete = oldChunks.filter(chunk => 
  chunk.endsWith('.') || chunk.endsWith('!') || chunk.endsWith('?')
).length;
const newComplete = newChunks.filter(chunk => chunk.metadata.isComplete).length;
const caseComplete = caseStudyChunks.filter(chunk => chunk.metadata.isComplete).length;

console.log(`\nChunks ho√†n ch·ªânh:`);
console.log(`- Thu·∫≠t to√°n c≈©: ${oldComplete}/${oldChunks.length} (${(oldComplete/oldChunks.length*100).toFixed(1)}%)`);
console.log(`- Academic: ${newComplete}/${newChunks.length} (${(newComplete/newChunks.length*100).toFixed(1)}%)`);
console.log(`- Case study: ${caseComplete}/${caseStudyChunks.length} (${(caseComplete/caseStudyChunks.length*100).toFixed(1)}%)`);

console.log('\n‚úÖ Ho√†n th√†nh so s√°nh!');
