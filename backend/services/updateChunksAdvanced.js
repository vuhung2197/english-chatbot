import { academicChunking, caseStudyChunking } from '../utils/advancedChunking.js';
import { createHash } from '../utils/hash.js';
import pool from '../db.js';
import { getEmbedding } from './embeddingVector.js';

/**
 * C·∫≠p nh·∫≠t chunks v·ªõi thu·∫≠t to√°n th√¥ng minh
 * @param {number|string} id - ID c·ªßa b·∫£n ghi ki·∫øn th·ª©c cha
 * @param {string} title - Ti√™u ƒë·ªÅ ki·∫øn th·ª©c
 * @param {string} content - N·ªôi dung ki·∫øn th·ª©c
 * @param {string} chunkingType - Lo·∫°i chunking: 'academic' | 'case_study' | 'auto'
 */
export async function updateChunksAdvanced(id, title, content, chunkingType = 'auto') {
  console.log(`üîÑ Updating chunks for knowledge ${id} with ${chunkingType} chunking...`);
  
  // Ch·ªçn thu·∫≠t to√°n chunking
  let chunks;
  switch (chunkingType) {
    case 'academic':
      chunks = academicChunking(content);
      break;
    case 'case_study':
      chunks = caseStudyChunking(content);
      break;
    case 'auto':
    default:
      // T·ª± ƒë·ªông ch·ªçn d·ª±a tr√™n n·ªôi dung
      if (content.toLowerCase().includes('case study') || 
          content.toLowerCase().includes('v√≠ d·ª•') ||
          content.toLowerCase().includes('·ª©ng d·ª•ng')) {
        chunks = caseStudyChunking(content);
        console.log('üìö Detected case study content, using case study chunking');
      } else {
        chunks = academicChunking(content);
        console.log('üìñ Using academic chunking');
      }
      break;
  }

  console.log(`üìä Generated ${chunks.length} semantic chunks`);

  let processedChunks = 0;
  let skippedChunks = 0;
  let errorChunks = 0;

  for (const chunk of chunks) {
    try {
      const hash = createHash(chunk.content);
      
      // Ki·ªÉm tra n·∫øu ƒë√£ t·ªìn t·∫°i chunk n√†y
      const [exists] = await pool.execute(
        'SELECT id FROM knowledge_chunks WHERE hash = ? LIMIT 1',
        [hash]
      );
      
      if (exists.length > 0) {
        skippedChunks++;
        console.log(`‚è≠Ô∏è  Skipped existing chunk (${chunk.metadata.wordCount} words)`);
        continue;
      }

      // T·∫°o embedding
      const embedding = await getEmbedding(chunk.content);
      
      // L∆∞u chunk v·ªõi metadata
      await pool.execute(
        `INSERT INTO knowledge_chunks 
          (parent_id, title, content, embedding, token_count, hash) 
         VALUES (?, ?, ?, ?, ?, ?)`,
        [id, title, chunk.content, JSON.stringify(embedding), chunk.metadata.wordCount, hash]
      );

      processedChunks++;
      console.log(`‚úÖ Processed chunk ${processedChunks}/${chunks.length} (${chunk.metadata.wordCount} words, ${chunk.metadata.boundary} boundary)`);
      
    } catch (error) {
      errorChunks++;
      console.error(`‚ùå Error processing chunk:`, error.message);
    }
  }

  console.log(`\nüìà Summary:`);
  console.log(`   - Processed: ${processedChunks} chunks`);
  console.log(`   - Skipped: ${skippedChunks} chunks`);
  console.log(`   - Errors: ${errorChunks} chunks`);
  console.log(`   - Total: ${chunks.length} chunks`);

  return {
    total: chunks.length,
    processed: processedChunks,
    skipped: skippedChunks,
    errors: errorChunks
  };
}

/**
 * Re-chunk to√†n b·ªô knowledge base v·ªõi thu·∫≠t to√°n m·ªõi
 */
export async function reChunkAllKnowledge() {
  console.log('üöÄ Starting re-chunking of all knowledge with advanced algorithm...');
  
  try {
    // L·∫•y t·∫•t c·∫£ knowledge base
    const [rows] = await pool.execute(
      'SELECT id, title, content FROM knowledge_base WHERE content IS NOT NULL AND content != ""'
    );

    console.log(`üìö Found ${rows.length} knowledge records`);

    if (rows.length === 0) {
      console.log('‚ö†Ô∏è  No records found to process');
      return;
    }

    let totalProcessed = 0;
    let totalSkipped = 0;
    let totalErrors = 0;

    for (const row of rows) {
      try {
        console.log(`\nüìù Processing: ${row.title}`);
        
        // X√≥a chunks c≈©
        await pool.execute('DELETE FROM knowledge_chunks WHERE parent_id = ?', [row.id]);
        console.log(`üóëÔ∏è  Deleted old chunks for knowledge ${row.id}`);

        // T·∫°o chunks m·ªõi
        const result = await updateChunksAdvanced(row.id, row.title, row.content, 'auto');
        
        totalProcessed += result.processed;
        totalSkipped += result.skipped;
        totalErrors += result.errors;

      } catch (error) {
        console.error(`‚ùå Error processing knowledge ${row.id}:`, error.message);
        totalErrors++;
      }
    }

    console.log(`\nüéâ Re-chunking completed!`);
    console.log(`   - Total processed: ${totalProcessed} chunks`);
    console.log(`   - Total skipped: ${totalSkipped} chunks`);
    console.log(`   - Total errors: ${totalErrors} chunks`);

  } catch (error) {
    console.error('‚ùå Fatal error in re-chunking:', error);
    throw error;
  }
}

/**
 * So s√°nh chunks c≈© v√† m·ªõi cho m·ªôt knowledge
 */
export async function compareChunkingMethods(knowledgeId) {
  console.log(`üîç Comparing chunking methods for knowledge ${knowledgeId}...`);
  
  try {
    // L·∫•y knowledge content
    const [rows] = await pool.execute(
      'SELECT id, title, content FROM knowledge_base WHERE id = ?',
      [knowledgeId]
    );

    if (rows.length === 0) {
      throw new Error(`Knowledge ${knowledgeId} not found`);
    }

    const { title, content } = rows[0];
    
    // Test thu·∫≠t to√°n c≈©
    const { splitIntoSemanticChunks } = await import('../utils/chunking.js');
    const oldChunks = splitIntoSemanticChunks(content, 100);
    
    // Test thu·∫≠t to√°n m·ªõi
    const newChunks = academicChunking(content);
    
    console.log(`\nüìä Results for "${title}":`);
    console.log(`   - Old method: ${oldChunks.length} chunks`);
    console.log(`   - New method: ${newChunks.length} chunks`);
    
    const oldAvgWords = oldChunks.reduce((sum, chunk) => sum + chunk.split(/\s+/).length, 0) / oldChunks.length;
    const newAvgWords = newChunks.reduce((sum, chunk) => sum + chunk.metadata.wordCount, 0) / newChunks.length;
    
    console.log(`   - Old avg words: ${oldAvgWords.toFixed(1)}`);
    console.log(`   - New avg words: ${newAvgWords.toFixed(1)}`);
    
    const oldComplete = oldChunks.filter(chunk => 
      chunk.endsWith('.') || chunk.endsWith('!') || chunk.endsWith('?')
    ).length;
    const newComplete = newChunks.filter(chunk => chunk.metadata.isComplete).length;
    
    console.log(`   - Old complete: ${oldComplete}/${oldChunks.length} (${(oldComplete/oldChunks.length*100).toFixed(1)}%)`);
    console.log(`   - New complete: ${newComplete}/${newChunks.length} (${(newComplete/newChunks.length*100).toFixed(1)}%)`);

    return {
      old: {
        count: oldChunks.length,
        avgWords: oldAvgWords,
        complete: oldComplete,
        completeRate: oldComplete / oldChunks.length
      },
      new: {
        count: newChunks.length,
        avgWords: newAvgWords,
        complete: newComplete,
        completeRate: newComplete / newChunks.length
      }
    };

  } catch (error) {
    console.error('‚ùå Error comparing chunking methods:', error);
    throw error;
  }
}
