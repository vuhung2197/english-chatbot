# ğŸ”„ Luá»“ng Xá»­ LÃ½ Advanced RAG - Chi Tiáº¿t

## ğŸ“‹ Tá»•ng Quan

**Advanced RAG** lÃ  há»‡ thá»‘ng RAG nÃ¢ng cao vá»›i kháº£ nÄƒng xá»­ lÃ½ cÃ¢u há»i phá»©c táº¡p báº±ng cÃ¡ch:
- **Multi-Stage Retrieval**: Láº¥y chunks theo nhiá»u giai Ä‘oáº¡n vá»›i threshold khÃ¡c nhau
- **Semantic Clustering**: NhÃ³m chunks theo chá»§ Ä‘á» Ä‘á»ƒ tÃ¬m má»‘i liÃªn káº¿t
- **Multi-Hop Reasoning**: TÃ¬m thÃ´ng tin liÃªn quan dá»±a trÃªn chunks Ä‘Ã£ cÃ³
- **Context Re-ranking**: Sáº¯p xáº¿p láº¡i chunks theo relevance, coherence, vÃ  completeness
- **Context Fusion**: Káº¿t há»£p thÃ´ng minh cÃ¡c chunks thÃ nh context cÃ³ cáº¥u trÃºc

---

## ğŸ—ï¸ Luá»“ng Xá»­ LÃ½ Tá»•ng Thá»ƒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER QUESTION                             â”‚
â”‚                  {message, model, userId}                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  1. EMBEDDING GENERATION             â”‚
        â”‚  - Convert question â†’ vector          â”‚
        â”‚  - API: OpenAI text-embedding-3-small â”‚
        â”‚  - Output: questionEmbedding[1536]    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  2. ADAPTIVE RETRIEVAL               â”‚
        â”‚  - Analyze question complexity        â”‚
        â”‚  - Determine retrieval parameters     â”‚
        â”‚  - Output: retrievalParams           â”‚
        â”‚    {maxChunks, threshold,             â”‚
        â”‚     useMultiHop, useSemanticClustering}â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  3. MULTI-STAGE RETRIEVAL             â”‚
        â”‚  Stage 1: threshold=0.7, topK=5      â”‚
        â”‚  Stage 2: threshold=0.5, topK=8      â”‚
        â”‚  Stage 3: threshold=0.3, topK=12      â”‚
        â”‚  - Output: allChunks[]                â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  4. SEMANTIC CLUSTERING               â”‚
        â”‚  - Group chunks by topic              â”‚
        â”‚  - Calculate similarity matrix        â”‚
        â”‚  - Output: clusters[][]              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  5. MULTI-HOP REASONING (Optional)   â”‚
        â”‚  - Find related chunks               â”‚
        â”‚  - Build reasoning chains            â”‚
        â”‚  - Output: reasoningChains[]         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  6. CONTEXT RE-RANKING                â”‚
        â”‚  - Calculate relevance score          â”‚
        â”‚  - Calculate coherence score          â”‚
        â”‚  - Calculate completeness score       â”‚
        â”‚  - Combine: 0.4*relevance +           â”‚
        â”‚             0.3*coherence +            â”‚
        â”‚             0.3*completeness            â”‚
        â”‚  - Output: rerankedChunks[]           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  7. CONTEXT FUSION                   â”‚
        â”‚  - Group chunks by topic              â”‚
        â”‚  - Add reasoning chains               â”‚
        â”‚  - Output: fusedContext (string)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  8. CONTEXT TRUNCATION               â”‚
        â”‚  - Limit to 6000 chars               â”‚
        â”‚  - Preserve most relevant chunks     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  9. LLM GENERATION                   â”‚
        â”‚  - Build prompt vá»›i system + context  â”‚
        â”‚  - Call LLM (OpenAI/Ollama)          â”‚
        â”‚  - Timeout: 180s                     â”‚
        â”‚  - Output: reply (string)            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  10. MARKDOWN FORMATTING             â”‚
        â”‚  - Convert to markdown                â”‚
        â”‚  - Format headers, lists, code        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  11. RESPONSE                       â”‚
        â”‚  {                                   â”‚
        â”‚    reply,                            â”‚
        â”‚    reasoning_steps,                  â”‚
        â”‚    chunks_used[],                    â”‚
        â”‚    metadata{}                        â”‚
        â”‚  }                                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Chi Tiáº¿t Tá»«ng BÆ°á»›c

### **BÆ°á»›c 1: Embedding Generation**

**File**: `backend/controllers/advancedChatController.js` (line 101-112)  
**Service**: `backend/services/embeddingVector.js`

```javascript
// Input
const message = "So sÃ¡nh RAG vÃ  fine-tuning";

// Process
const questionEmbedding = await getEmbedding(message);
// API Call: POST https://api.openai.com/v1/embeddings
// Model: text-embedding-3-small
// Cost: ~$0.001 per query

// Output
questionEmbedding = [0.012, -0.045, 0.089, ..., 0.023] // 1536 dimensions
```

**Äáº·c Ä‘iá»ƒm:**
- â±ï¸ **Latency**: 200-500ms
- ğŸ’° **Cost**: $0.02 per 1M tokens
- âŒ **Error Handling**: Return error náº¿u API fail

**Error Cases:**
- API timeout â†’ Return error message
- Invalid API key â†’ Return error message
- Network error â†’ Return error message

---

### **BÆ°á»›c 2: Adaptive Retrieval**

**File**: `backend/services/advancedRAGFixed.js` (line 215-254)  
**Function**: `adaptiveRetrieval(question, questionEmbedding)`

```javascript
// Input
const question = "So sÃ¡nh RAG vÃ  fine-tuning";
const questionEmbedding = [0.012, -0.045, ...];

// Process
const complexity = analyzeQuestionComplexity(question);
// Checks for:
// - "so sÃ¡nh", "khÃ¡c biá»‡t", "má»‘i quan há»‡" â†’ isComplex = true
// - Multiple "vÃ ", "vá»›i", "káº¿t há»£p" â†’ hasMultipleTopics = true
// - "táº¡i sao", "nhÆ° tháº¿ nÃ o", "giáº£i thÃ­ch" â†’ requiresReasoning = true

// Output
const retrievalParams = {
  maxChunks: 10,           // Increased if complex
  threshold: 0.3,          // Lower if complex
  useMultiHop: true,       // If complex or requires reasoning
  useSemanticClustering: true // If has multiple topics
};
```

**Logic:**
- **Simple question**: maxChunks=5, threshold=0.5, useMultiHop=false
- **Complex question**: maxChunks=10, threshold=0.3, useMultiHop=true
- **Multiple topics**: maxChunks=15, useSemanticClustering=true
- **Requires reasoning**: useMultiHop=true, useSemanticClustering=true

---

### **BÆ°á»›c 3: Multi-Stage Retrieval**

**File**: `backend/services/advancedRAGFixed.js` (line 14-52)  
**Function**: `multiStageRetrieval(questionEmbedding, question, maxChunks)`

```javascript
// Input
const questionEmbedding = [0.012, -0.045, ...];
const question = "So sÃ¡nh RAG vÃ  fine-tuning";
const maxChunks = 10;

// Process
const stages = [
  { topK: 5, threshold: 0.7, name: 'high_similarity' },
  { topK: 8, threshold: 0.5, name: 'medium_similarity' },
  { topK: 12, threshold: 0.3, name: 'low_similarity' }
];

// For each stage:
// 1. Query database: SELECT ... LIMIT topK*2
// 2. Calculate cosine similarity for each chunk
// 3. Filter by threshold
// 4. Sort by score
// 5. Take topK

// Output
const allChunks = [
  {
    id: 123,
    title: "RAG Overview",
    content: "Retrieval-Augmented Generation...",
    score: 0.85,
    retrieval_stage: "high_similarity",
    embedding: [0.015, -0.042, ...]
  },
  // ... more chunks
].slice(0, maxChunks); // Max 10 chunks
```

**Äáº·c Ä‘iá»ƒm:**
- â±ï¸ **Latency**: 50-200ms per stage = 150-600ms total
- ğŸ“Š **Coverage**: Progressive retrieval ensures good coverage
- ğŸ”„ **Deduplication**: Remove duplicates by (id, title)

**Error Handling:**
- If stage fails â†’ Continue with next stage
- If all stages fail â†’ Return empty array

---

### **BÆ°á»›c 4: Semantic Clustering**

**File**: `backend/services/advancedRAGFixed.js` (line 58-124)  
**Function**: `semanticClustering(chunks, questionEmbedding)`

```javascript
// Input
const chunks = [
  { id: 1, title: "RAG", content: "...", embedding: [...] },
  { id: 2, title: "Fine-tuning", content: "...", embedding: [...] },
  { id: 3, title: "RAG Applications", content: "...", embedding: [...] }
];

// Process
// 1. Get embeddings for each chunk (if not available)
for (let i = 0; i < chunks.length; i++) {
  const embedding = await getEmbedding(chunks[i].content); // âš ï¸ Expensive!
  chunkEmbeddings.push(embedding);
}

// 2. Calculate similarity matrix
const similarityMatrix = [];
for (let i = 0; i < chunks.length; i++) {
  for (let j = 0; j < chunks.length; j++) {
    similarityMatrix[i][j] = cosineSimilarity(
      chunkEmbeddings[i], 
      chunkEmbeddings[j]
    );
  }
}

// 3. Cluster chunks with similarity > 0.6
const clusters = [];
// Group chunks that are similar to each other

// Output
const clusters = [
  [chunk1, chunk3], // RAG-related chunks
  [chunk2]          // Fine-tuning chunk
];
```

**Äáº·c Ä‘iá»ƒm:**
- â±ï¸ **Latency**: 1-3s (depends on number of chunks)
- ğŸ’° **Cost**: N chunks Ã— $0.001 = High cost!
- âš ï¸ **Issue**: Gá»i embedding API cho má»—i chunk (cÃ³ thá»ƒ reuse existing embeddings)

**Error Handling:**
- If embedding fails â†’ Use existing embedding if available
- If clustering fails â†’ Return single cluster with all chunks

---

### **BÆ°á»›c 5: Multi-Hop Reasoning (Optional)**

**File**: `backend/services/advancedRAGFixed.js` (line 130-161)  
**Function**: `multiHopReasoning(initialChunks, questionEmbedding, question)`

```javascript
// Input
const initialChunks = [chunk1, chunk2, chunk3]; // Top 3 chunks
const questionEmbedding = [0.012, -0.045, ...];
const question = "So sÃ¡nh RAG vÃ  fine-tuning";

// Process (only if retrievalParams.useMultiHop === true)
// For each initial chunk:
for (const chunk of initialChunks.slice(0, 3)) {
  // 1. Find related chunks
  const relatedChunks = await findRelatedChunks(chunk, 3);
  // Query: SELECT ... WHERE id != chunk.id
  // Calculate similarity with chunk.embedding
  // Filter similarity > 0.4
  
  // 2. Calculate reasoning score
  const reasoningScore = calculateReasoningScore(
    chunk, 
    relatedChunks, 
    questionEmbedding
  );
  // Formula: baseScore * 0.6 + avgRelatedScore * 0.4
  
  // 3. Build reasoning chain
  const chain = {
    source_chunk: chunk,
    related_chunks: relatedChunks,
    reasoning_score: reasoningScore
  };
  
  reasoningChains.push(chain);
}

// Sort by reasoning score
reasoningChains.sort((a, b) => b.reasoning_score - a.reasoning_score);

// Output
const reasoningChains = [
  {
    source_chunk: chunk1,
    related_chunks: [chunk4, chunk5],
    reasoning_score: 0.78
  },
  // ... top 3 chains
];
```

**Äáº·c Ä‘iá»ƒm:**
- â±ï¸ **Latency**: 200-500ms per chunk = 600-1500ms total
- ğŸ”— **Purpose**: Find connections between chunks
- ğŸ“Š **Limitation**: Limited to top 3 chunks to avoid timeout

**Error Handling:**
- If reasoning fails for one chunk â†’ Continue with others
- If all fail â†’ Return empty array (continue without reasoning)

---

### **BÆ°á»›c 6: Context Re-ranking**

**File**: `backend/services/advancedRAGFixed.js` (line 260-291)  
**Function**: `rerankContext(chunks, questionEmbedding, question)`

```javascript
// Input
const chunks = [chunk1, chunk2, ..., chunk10];
const questionEmbedding = [0.012, -0.045, ...];
const question = "So sÃ¡nh RAG vÃ  fine-tuning";

// Process
const rerankedChunks = chunks.map(chunk => {
  // 1. Relevance Score (from vector search)
  const relevanceScore = chunk.score || 0; // 0.85
  
  // 2. Coherence Score (similarity with other chunks)
  const coherenceScore = calculateCoherenceScore(chunk, chunks);
  // Average similarity vá»›i cÃ¡c chunks khÃ¡c
  // Formula: avg(cosineSimilarity(chunk.embedding, other.embedding))
  
  // 3. Completeness Score (keyword matching)
  const completenessScore = calculateCompletenessScore(chunk, question);
  // Count matching words / total words
  // Formula: matchedWords.length / questionWords.length
  
  // 4. Combined Score
  const finalScore = (
    relevanceScore * 0.4 +    // 40% weight
    coherenceScore * 0.3 +     // 30% weight
    completenessScore * 0.3    // 30% weight
  );
  
  return {
    ...chunk,
    final_score: finalScore,
    relevance_score: relevanceScore,
    coherence_score: coherenceScore,
    completeness_score: completenessScore
  };
}).sort((a, b) => b.final_score - a.final_score);

// Output
const rerankedChunks = [
  { id: 1, title: "RAG", final_score: 0.82, ... },
  { id: 2, title: "Fine-tuning", final_score: 0.78, ... },
  // ... sorted by final_score
];
```

**Äáº·c Ä‘iá»ƒm:**
- â±ï¸ **Latency**: 10-50ms (fast, in-memory calculation)
- ğŸ“Š **Weight Distribution**: 40% relevance, 30% coherence, 30% completeness
- ğŸ¯ **Purpose**: Ensure most relevant chunks come first

**Limitations:**
- Completeness score chá»‰ dÃ¹ng keyword matching (khÃ´ng semantic)
- CÃ³ thá»ƒ improve báº±ng BM25 hoáº·c TF-IDF

---

### **BÆ°á»›c 7: Context Fusion**

**File**: `backend/services/advancedRAGFixed.js` (line 167-209)  
**Function**: `fuseContext(chunks, reasoningChains, question)`

```javascript
// Input
const chunks = [chunk1, chunk2, ...]; // Re-ranked chunks
const reasoningChains = [chain1, chain2, ...]; // Optional
const question = "So sÃ¡nh RAG vÃ  fine-tuning";

// Process
let context = `# ThÃ´ng tin chÃ­nh:\n\n`;

// 1. Group chunks by topic
const topicGroups = groupChunksByTopic(chunks);
// Topics: "NLP", "AI", "Machine Learning", "Chatbot", "Vector Search", "KhÃ¡c"
// Extract topic from title + content

// 2. Add chunks by topic
for (const [topic, topicChunks] of Object.entries(topicGroups)) {
  context += `## ${topic}:\n`;
  
  topicChunks.forEach((chunk, index) => {
    context += `### ${chunk.title || `Chunk ${index + 1}`}\n`;
    context += `${chunk.content}\n\n`;
  });
}

// 3. Add reasoning chains (if available)
if (reasoningChains && reasoningChains.length > 0) {
  context += `# Má»‘i liÃªn káº¿t thÃ´ng tin:\n\n`;
  
  reasoningChains.forEach((chain, index) => {
    context += `## LiÃªn káº¿t ${index + 1}:\n`;
    context += `**Nguá»“n chÃ­nh:** ${chain.source_chunk.title}\n`;
    context += `**Ná»™i dung:** ${chain.source_chunk.content}\n\n`;
    
    if (chain.related_chunks && chain.related_chunks.length > 0) {
      context += `**ThÃ´ng tin liÃªn quan:**\n`;
      chain.related_chunks.forEach(related => {
        context += `- ${related.title}: ${related.content.substring(0, 200)}...\n`;
      });
      context += `\n`;
    }
  });
}

// Output
const fusedContext = `
# ThÃ´ng tin chÃ­nh:

## AI:
### RAG Overview
Retrieval-Augmented Generation...

### Fine-tuning Overview
Fine-tuning is...

# Má»‘i liÃªn káº¿t thÃ´ng tin:

## LiÃªn káº¿t 1:
**Nguá»“n chÃ­nh:** RAG Overview
**Ná»™i dung:** ...
**ThÃ´ng tin liÃªn quan:**
- RAG Applications: ...
`;
```

**Äáº·c Ä‘iá»ƒm:**
- ğŸ“ **Format**: Structured markdown vá»›i headers
- ğŸ¯ **Organization**: Grouped by topic for clarity
- ğŸ”— **Reasoning**: Include connections between chunks

**Error Handling:**
- If fusion fails â†’ Fallback to simple context: `**${title}**: ${content}`

---

### **BÆ°á»›c 8: Context Truncation**

**File**: `backend/controllers/advancedChatController.js` (line 283-291)  
**Function**: `askAdvancedChatGPT()`

```javascript
// Input
const fusedContext = "..."; // Potentially very long (>10000 chars)

// Process
const maxContextLength = 6000; // Hard-coded limit
const truncatedContext = fusedContext.length > maxContextLength 
  ? `${fusedContext.substring(0, maxContextLength)}...` 
  : fusedContext;

// âš ï¸ Issue: Truncation tá»« Ä‘áº§u â†’ CÃ³ thá»ƒ máº¥t thÃ´ng tin quan trá»ng
// Better: Keep most relevant chunks first (Ä‘Ã£ Ä‘Æ°á»£c re-ranked)

// Output
const truncatedContext = "..."; // Max 6000 chars
```

**Äáº·c Ä‘iá»ƒm:**
- ğŸ“ **Limit**: 6000 characters (hard-coded)
- âš ï¸ **Issue**: Simple truncation, khÃ´ng intelligent
- ğŸ’¡ **Improvement**: Smart truncation based on chunk scores

---

### **BÆ°á»›c 9: LLM Generation**

**File**: `backend/controllers/advancedChatController.js` (line 283-319)  
**Function**: `askAdvancedChatGPT(question, context, systemPrompt, model)`

```javascript
// Input
const question = "So sÃ¡nh RAG vÃ  fine-tuning";
const context = "..."; // Truncated context (6000 chars)
const systemPrompt = `Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn nghiá»‡p...`;
const model = {
  name: "gpt-4o",
  url: "https://api.openai.com/v1",
  temperature: 0.3,
  maxTokens: 800
};

// Process
const prompt = `# CÃ¢u há»i: ${question}

# ThÃ´ng tin tham kháº£o:
${truncatedContext}

# HÆ°á»›ng dáº«n:
HÃ£y phÃ¢n tÃ­ch cÃ¢u há»i vÃ  sá»­ dá»¥ng thÃ´ng tin tham kháº£o Ä‘á»ƒ táº¡o cÃ¢u tráº£ lá»i toÃ n diá»‡n.
Káº¿t há»£p thÃ´ng tin tá»« nhiá»u nguá»“n má»™t cÃ¡ch logic vÃ  cÃ³ cáº¥u trÃºc.`;

const messages = [
  { 
    role: 'system', 
    content: systemPrompt.substring(0, 4000) // Max 4000 chars
  },
  { 
    role: 'user', 
    content: prompt.substring(0, 8000) // Max 8000 chars
  }
];

// Call LLM with timeout
const timeoutPromise = new Promise((_, reject) => 
  setTimeout(() => reject(new Error('LLM call timeout')), 180000)
);

const llmPromise = callLLM(model, messages, 0.3, 800);
const reply = await Promise.race([llmPromise, timeoutPromise]);

// Output
const reply = `
RAG (Retrieval-Augmented Generation) vÃ  Fine-tuning lÃ  hai phÆ°Æ¡ng phÃ¡p...
`;
```

**Äáº·c Ä‘iá»ƒm:**
- â±ï¸ **Latency**: 1-3s (depends on LLM)
- ğŸ’° **Cost**: ~$0.003 per query (depends on tokens)
- â±ï¸ **Timeout**: 180s (3 minutes)
- ğŸ”’ **Error Handling**: Comprehensive error messages

**Error Handling:**
- API error â†’ "Lá»—i káº¿t ná»‘i vá»›i model: ..."
- Timeout â†’ "Model máº¥t quÃ¡ nhiá»u thá»i gian..."
- Other errors â†’ "Xin lá»—i, tÃ´i gáº·p sá»± cá»‘..."

---

### **BÆ°á»›c 10: Markdown Formatting**

**File**: `backend/controllers/advancedChatController.js` (line 24-65)  
**Function**: `toAdvancedMarkdown(text)`

```javascript
// Input
const reply = `
RAG vÃ  Fine-tuning lÃ  hai phÆ°Æ¡ng phÃ¡p khÃ¡c nhau.

- RAG: Sá»­ dá»¥ng external knowledge
- Fine-tuning: Train model vá»›i custom data

\`\`\`python
def rag_example():
    return "Hello"
\`\`\`
`;

// Process
// 1. Split into paragraphs
// 2. Detect headers (# ## ###)
// 3. Detect lists (- * â€¢)
// 4. Detect code blocks (```)
// 5. Format accordingly

// Output
const markdown = `
RAG vÃ  Fine-tuning lÃ  hai phÆ°Æ¡ng phÃ¡p khÃ¡c nhau.

- RAG: Sá»­ dá»¥ng external knowledge
- Fine-tuning: Train model vá»›i custom data

\`\`\`python
def rag_example():
    return "Hello"
\`\`\`
`;
```

**Äáº·c Ä‘iá»ƒm:**
- ğŸ“ **Format**: Clean markdown output
- ğŸ¨ **Styling**: Headers, lists, code blocks
- âœ… **Preservation**: Keeps original structure

---

### **BÆ°á»›c 11: Response**

**File**: `backend/controllers/advancedChatController.js` (line 248-268)

```javascript
// Output
res.json({ 
  reply: toAdvancedMarkdown(reply),              // Formatted response
  reasoning_steps: [                              // Debug info
    `Retrieved ${allChunks.length} chunks using multi-stage retrieval`,
    `Created ${clusters.length} semantic clusters`,
    `Generated ${reasoningChains.length} reasoning chains`,
    `Fused context with ${fusedContext.length} characters`,
    `Generated response using advanced RAG with model ${model.name}`
  ],
  chunks_used: rerankedChunks.map(c => ({        // Chunks used for response
    id: c.id,
    title: c.title,
    content: c.content.substring(0, 200) + '...',
    score: c.final_score || c.score,
    stage: c.retrieval_stage,
    source: c.source || 'unknown',
    chunk_index: c.chunk_index || 0
  })),
  metadata: {                                    // Processing metadata
    total_chunks: allChunks.length,
    clusters: clusters.length,
    reasoning_chains: reasoningChains.length,
    processing_time: t1 - t0,                    // Total processing time
    model_used: model.name,
    context_length: fusedContext.length
  }
});
```

**Response Structure:**
```json
{
  "reply": "Markdown formatted response...",
  "reasoning_steps": [
    "Retrieved 10 chunks...",
    "Created 3 semantic clusters...",
    "..."
  ],
  "chunks_used": [
    {
      "id": 123,
      "title": "RAG Overview",
      "content": "Retrieval-Augmented Generation...",
      "score": 0.85,
      "stage": "high_similarity",
      "source": "unknown",
      "chunk_index": 0
    }
  ],
  "metadata": {
    "total_chunks": 10,
    "clusters": 3,
    "reasoning_chains": 2,
    "processing_time": 3456,
    "model_used": "gpt-4o",
    "context_length": 5842
  }
}
```

---

## â±ï¸ Performance Timeline

### **Typical Flow (Simple Question)**
```
Step 1: Embedding          200ms
Step 2: Adaptive Retrieval 10ms
Step 3: Multi-Stage       150ms
Step 4: Clustering        200ms (skip if <3 chunks)
Step 5: Reasoning         0ms (skip)
Step 6: Re-ranking        20ms
Step 7: Fusion             10ms
Step 8: Truncation         5ms
Step 9: LLM               1500ms
Step 10: Markdown          5ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                    ~2100ms (2.1s)
```

### **Complex Flow (Complex Question)**
```
Step 1: Embedding          300ms
Step 2: Adaptive Retrieval 15ms
Step 3: Multi-Stage        300ms (more chunks)
Step 4: Clustering        2000ms (N embedding calls)
Step 5: Reasoning         1000ms (3 chains)
Step 6: Re-ranking        50ms (more chunks)
Step 7: Fusion             20ms
Step 8: Truncation         10ms
Step 9: LLM               2500ms (longer context)
Step 10: Markdown          10ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                    ~6205ms (6.2s)
```

---

## ğŸ” Error Handling Flow

```
User Question
    â†“
Try Block (Main)
    â”œâ”€â†’ Step 1: Embedding
    â”‚   â”œâ”€â†’ Success: Continue
    â”‚   â””â”€â†’ Error: Return error message
    â”‚
    â”œâ”€â†’ Step 2: Adaptive Retrieval
    â”‚   â”œâ”€â†’ Success: Continue
    â”‚   â””â”€â†’ Error: Use default params
    â”‚
    â”œâ”€â†’ Step 3: Multi-Stage Retrieval
    â”‚   â”œâ”€â†’ Success: Continue
    â”‚   â”œâ”€â†’ Empty chunks: Return "No knowledge"
    â”‚   â””â”€â†’ Error: Return empty chunks
    â”‚
    â”œâ”€â†’ Step 4: Semantic Clustering
    â”‚   â”œâ”€â†’ Success: Continue
    â”‚   â””â”€â†’ Error: Fallback to single cluster
    â”‚
    â”œâ”€â†’ Step 5: Multi-Hop Reasoning
    â”‚   â”œâ”€â†’ Success: Continue
    â”‚   â””â”€â†’ Error: Continue without reasoning
    â”‚
    â”œâ”€â†’ Step 6: Context Re-ranking
    â”‚   â”œâ”€â†’ Success: Continue
    â”‚   â””â”€â†’ Error: Use original chunks
    â”‚
    â”œâ”€â†’ Step 7: Context Fusion
    â”‚   â”œâ”€â†’ Success: Continue
    â”‚   â””â”€â†’ Error: Fallback to simple context
    â”‚
    â”œâ”€â†’ Step 9: LLM Generation
    â”‚   â”œâ”€â†’ Success: Continue
    â”‚   â”œâ”€â†’ API Error: Return error message
    â”‚   â”œâ”€â†’ Timeout: Return timeout message
    â”‚   â””â”€â†’ Other Error: Return generic error
    â”‚
    â””â”€â†’ Response
        â”‚
        â””â”€â†’ Catch Block (Overall)
            â””â”€â†’ Return generic error message
```

---

## ğŸ“Š Data Structures

### **Chunk Structure**
```typescript
interface Chunk {
  id: number;
  title: string;
  content: string;
  embedding: number[];              // 1536 dimensions
  score?: number;                    // Similarity score
  final_score?: number;              // Combined re-ranking score
  relevance_score?: number;          // From vector search
  coherence_score?: number;          // From coherence calculation
  completeness_score?: number;       // From keyword matching
  retrieval_stage?: string;          // "high_similarity" | "medium" | "low"
  source?: string;                   // Source document
  chunk_index?: number;              // Index in document
}
```

### **Retrieval Parameters**
```typescript
interface RetrievalParams {
  maxChunks: number;                 // 5 | 10 | 15
  threshold: number;                 // 0.3 | 0.5
  useMultiHop: boolean;              // true | false
  useSemanticClustering: boolean;    // true | false
}
```

### **Reasoning Chain**
```typescript
interface ReasoningChain {
  source_chunk: Chunk;
  related_chunks: Chunk[];
  reasoning_score: number;           // 0-1
}
```

### **Complexity Analysis**
```typescript
interface Complexity {
  isComplex: boolean;                // Has "so sÃ¡nh", "khÃ¡c biá»‡t"
  hasMultipleTopics: boolean;        // Multiple "vÃ ", "vá»›i"
  requiresReasoning: boolean;         // Has "táº¡i sao", "nhÆ° tháº¿ nÃ o"
}
```

---

## ğŸ’¡ Optimization Opportunities

### **1. Embedding Cache** â­â­â­
- **Current**: Gá»i API má»—i query
- **Improvement**: Cache embeddings vá»›i Redis
- **Impact**: 70-90% cost reduction

### **2. Reuse Chunk Embeddings** â­â­â­
- **Current**: Gá»i API cho má»—i chunk trong clustering
- **Improvement**: Reuse existing chunk embeddings
- **Impact**: 100% cost savings cho clustering

### **3. Smart Context Truncation** â­â­
- **Current**: Simple substring truncation
- **Improvement**: Keep most relevant chunks first
- **Impact**: Better quality, no info loss

### **4. BM25 Completeness Score** â­â­
- **Current**: Simple keyword matching
- **Improvement**: Use BM25 scoring
- **Impact**: 10-20% better retrieval accuracy

### **5. Parallel Processing** â­
- **Current**: Sequential processing
- **Improvement**: Parallel stage retrieval
- **Impact**: 30-50% faster

---

## ğŸ”— Related Files

- **Controller**: `backend/controllers/advancedChatController.js`
- **Service**: `backend/services/advancedRAGFixed.js`
- **Embedding**: `backend/services/embeddingVector.js`
- **Vector Search**: `backend/services/vectorDatabase.js`
- **LLM Call**: `backend/controllers/chatController.js` (callLLM)

---

**Document Version**: 1.0  
**Last Updated**: 2024  
**Maintainer**: Development Team

